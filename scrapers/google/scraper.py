"""
Google Career èŒä½æŠ“å–å™¨
ä½¿ç”¨Playwrightè‡ªåŠ¨åŒ–æŠ“å–Google Careerç½‘ç«™çš„èŒä½ä¿¡æ¯
æŒ‰ç…§è¯¦ç»†éœ€æ±‚å®ç°ï¼šé€’å½’ç‚¹å‡»job cardsï¼Œæå–è¯¦ç»†èŒä½ä¿¡æ¯
"""

import asyncio
import logging
import json
import re
from typing import List, Dict, Any, Optional
from datetime import datetime
from playwright.async_api import async_playwright, Page, Browser
from urllib.parse import urljoin, urlparse


class GoogleCareerScraper:
    """Google CareerèŒä½æŠ“å–å™¨"""
    
    def __init__(self, website_config: Dict[str, Any], global_config: Dict[str, Any]):
        """
        åˆå§‹åŒ–æŠ“å–å™¨
        
        Args:
            website_config: ç½‘ç«™ä¸“ç”¨é…ç½®
            global_config: å…¨å±€é…ç½®
        """
        self.website_config = website_config
        self.global_config = global_config
        self.logger = logging.getLogger("job_tracker")
        
        # ç›®æ ‡ç½‘å€
        self.target_url = "https://www.google.com/about/careers/applications/jobs/results/?location=United%20States&location=Canada&sort_by=date"
        
        # å…¨å±€é…ç½®
        self.headless = global_config["global_settings"]["headless"]
        self.timeout = global_config["playwright_global"]["timeout"]
        self.user_agent = global_config["playwright_global"]["user_agent"]
        
        self.scraped_jobs = []
        self.current_index = 0
    
    async def scrape_jobs(self) -> List[Dict[str, Any]]:
        """
        ä¸»è¦çš„æŠ“å–æ–¹æ³•
        
        Returns:
            æŠ“å–åˆ°çš„èŒä½åˆ—è¡¨
        """
        try:
            async with async_playwright() as p:
                browser = await p.chromium.launch(
                    headless=self.headless,
                    args=['--no-sandbox', '--disable-dev-shm-usage']
                )
                
                page = await browser.new_page(user_agent=self.user_agent)
                
                # è®¾ç½®é¡µé¢è¶…æ—¶
                page.set_default_timeout(self.timeout)
                
                # æ‰§è¡Œæœç´¢å’ŒæŠ“å–
                await self._perform_search_and_scrape(page)
                
                await browser.close()
                
                self.logger.info(f"Google CareeræŠ“å–å®Œæˆï¼Œå…±è·å¾— {len(self.scraped_jobs)} ä¸ªèŒä½")
                return self.scraped_jobs
                
        except Exception as e:
            self.logger.error(f"Google CareeræŠ“å–è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
            return []
    
    async def _perform_search_and_scrape(self, page: Page) -> None:
        """
        æ‰§è¡Œæœç´¢å’ŒæŠ“å–çš„å®Œæ•´æµç¨‹
        
        Args:
            page: Playwrighté¡µé¢å¯¹è±¡
        """
        try:
            # æ­¥éª¤1: è®¿é—®ç›®æ ‡ç½‘å€
            self.logger.info(f"è®¿é—®é¡µé¢: {self.target_url}")
            await page.goto(self.target_url)
            
            # æ­¥éª¤2: ç­‰å¾…é¡µé¢åŠ è½½å¹¶ç‚¹å‡»ç¬¬ä¸€ä¸ªjob cardçš„"Learn more"
            self.logger.info("ç­‰å¾…Learn moreæŒ‰é’®åŠ è½½...")
            # await page.wait_for_selector('li.lLd3Je a[aria-label^="Learn more about"]', timeout=10000)
            self.logger.info("Learn moreæŒ‰é’®å·²åŠ è½½ï¼Œå‡†å¤‡ç‚¹å‡»")
            
            # ç‚¹å‡»ç¬¬ä¸€ä¸ªjob cardçš„Learn moreæŒ‰é’®
            first_learn_more = await page.query_selector('li.lLd3Je a[aria-label^="Learn more about"]')
            if first_learn_more:
                await first_learn_more.click()
                await page.wait_for_load_state('networkidle')
                self.logger.info("æˆåŠŸç‚¹å‡»ç¬¬ä¸€ä¸ªLearn moreæŒ‰é’®")
            else:
                self.logger.error("æœªæ‰¾åˆ°Learn moreæŒ‰é’®")
            
            # æ­¥éª¤3: å¼€å§‹å¤šé¡µé¢æŠ“å–
            await self._start_multi_page_scraping(page)
            
        except Exception as e:
            self.logger.error(f"æœç´¢å’ŒæŠ“å–è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
    
    async def _start_multi_page_scraping(self, page: Page) -> None:
        """
        å¼€å§‹å¤šé¡µé¢æŠ“å–
        
        Args:
            page: Playwrighté¡µé¢å¯¹è±¡
        """
        try:
            page_number = 1
            max_pages = 5  # æœ€å¤šæŠ“å–5é¡µï¼Œå¯ä»¥æ ¹æ®éœ€è¦è°ƒæ•´
            
            while page_number <= max_pages:
                self.logger.info(f"ğŸ” å¼€å§‹æŠ“å–ç¬¬ {page_number} é¡µ")
                
                # æŠ“å–å½“å‰é¡µé¢çš„æ‰€æœ‰job cards
                await self._start_recursive_scraping(page, page_number)
                
                # å°è¯•è·³è½¬åˆ°ä¸‹ä¸€é¡µ
                has_next_page = await self._goto_next_page(page)
                
                if not has_next_page:
                    self.logger.info("âœ… å·²åˆ°è¾¾æœ€åä¸€é¡µæˆ–æ— æ³•æ‰¾åˆ°ä¸‹ä¸€é¡µæŒ‰é’®")
                    break
                
                page_number += 1
                
                # ç­‰å¾…é¡µé¢åŠ è½½
                await page.wait_for_load_state('networkidle')
                await asyncio.sleep(2)  # é¢å¤–ç­‰å¾…ç¡®ä¿é¡µé¢å®Œå…¨åŠ è½½
                
        except Exception as e:
            self.logger.error(f"å¤šé¡µé¢æŠ“å–è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
    
    async def _start_recursive_scraping(self, page: Page, page_number: int = 1) -> None:
        """
        å¼€å§‹é€’å½’æŠ“å–å½“å‰é¡µé¢çš„æ‰€æœ‰job cards
        
        Args:
            page: Playwrighté¡µé¢å¯¹è±¡
            page_number: å½“å‰é¡µé¢ç¼–å·
        """
        try:
            # è·å–æ‰€æœ‰job cards
            job_cards = await page.query_selector_all('li.zE6MFb')
            total_jobs = len(job_cards)
            self.logger.info(f"ç¬¬ {page_number} é¡µå‘ç° {total_jobs} ä¸ªèŒä½")
            
            if total_jobs == 0:
                self.logger.warning(f"ç¬¬ {page_number} é¡µæ²¡æœ‰æ‰¾åˆ°ä»»ä½•èŒä½")
                return
            
            # å¼€å§‹é€’å½’ç‚¹å‡»å’Œæå–
            await self._click_and_scrape_job_card(page, 0)
            
        except Exception as e:
            self.logger.error(f"ç¬¬ {page_number} é¡µé€’å½’æŠ“å–æ—¶å‘ç”Ÿé”™è¯¯: {e}")
    
    async def _goto_next_page(self, page: Page) -> bool:
        """
        è·³è½¬åˆ°ä¸‹ä¸€é¡µ
        
        Args:
            page: Playwrighté¡µé¢å¯¹è±¡
            
        Returns:
            bool: æ˜¯å¦æˆåŠŸè·³è½¬åˆ°ä¸‹ä¸€é¡µ
        """
        try:
            self.logger.info("ğŸ”„ æŸ¥æ‰¾ä¸‹ä¸€é¡µæŒ‰é’®...")
            
            # ä½¿ç”¨JavaScriptæŸ¥æ‰¾å¹¶ç‚¹å‡»ä¸‹ä¸€é¡µæŒ‰é’®
            next_page_clicked = await page.evaluate('''
                () => {
                    const nextPageLink = document.querySelector('div[jsname="ViaHrd"] a');
                    if (nextPageLink) {
                        console.log('æ‰¾åˆ°ä¸‹ä¸€é¡µæŒ‰é’®ï¼Œå‡†å¤‡ç‚¹å‡»');
                        nextPageLink.click();
                        return true;
                    }
                    return false;
                }
            ''')
            
            if next_page_clicked:
                self.logger.info("âœ… æˆåŠŸç‚¹å‡»ä¸‹ä¸€é¡µæŒ‰é’®")
                return True
            else:
                self.logger.info("âŒ æœªæ‰¾åˆ°ä¸‹ä¸€é¡µæŒ‰é’®")
                return False
                
        except Exception as e:
            self.logger.error(f"è·³è½¬ä¸‹ä¸€é¡µæ—¶å‘ç”Ÿé”™è¯¯: {e}")
            return False
    
    async def _click_and_scrape_job_card(self, page: Page, index: int) -> None:
        """
        é€’å½’ç‚¹å‡»å¹¶æŠ“å–job cardæ•°æ®
        
        Args:
            page: Playwrighté¡µé¢å¯¹è±¡
            index: å½“å‰job cardç´¢å¼•
        """
        try:
            # é‡æ–°è·å–job cards (å› ä¸ºDOMå¯èƒ½å·²æ›´æ–°)
            job_cards = await page.query_selector_all('li.zE6MFb')
            
            if index >= len(job_cards):
                self.logger.info('âœ… æ‰€æœ‰job cardå·²å¤„ç†å®Œæ¯•')
                return
            
            # ç‚¹å‡»æŒ‡å®šç´¢å¼•çš„job card
            link = await job_cards[index].query_selector('a')
            if link:
                self.logger.info(f'ğŸ”˜ ç‚¹å‡»ç¬¬ {index + 1} ä¸ªjob card')
                await link.click()
                
                # ç­‰å¾…é¡µé¢åŠ è½½
                await page.wait_for_timeout(800)  # ç­‰å¾…800ms
                await page.wait_for_load_state('networkidle')
                
                # æå–æ•°æ®
                job_data = await self._extract_job_data(page)
                if job_data:
                    self.scraped_jobs.append(job_data)
                    self.logger.info(f'âœ… æˆåŠŸæå–ç¬¬ {index + 1} ä¸ªèŒä½æ•°æ®: {job_data.get("title", "Unknown")}')
                    self._log_progress(index + 1, len(job_cards))
                
                # é€’å½’è°ƒç”¨ä¸‹ä¸€ä¸ª
                await self._click_and_scrape_job_card(page, index + 1)
            else:
                self.logger.warning(f'âš ï¸ ç¬¬ {index + 1} ä¸ªjob cardæ²¡æœ‰é“¾æ¥ï¼Œè·³è¿‡')
                await self._click_and_scrape_job_card(page, index + 1)
                
        except Exception as e:
            self.logger.error(f'âŒ å¤„ç†ç¬¬ {index + 1} ä¸ªjob cardæ—¶å‡ºé”™: {e}')
            # ç»§ç»­å¤„ç†ä¸‹ä¸€ä¸ª
            await self._click_and_scrape_job_card(page, index + 1)
    
    async def _extract_job_data(self, page: Page) -> Optional[Dict[str, Any]]:
        """
        ä»è¯¦æƒ…é¡µé¢æå–èŒä½æ•°æ®
        
        Args:
            page: Playwrighté¡µé¢å¯¹è±¡
            
        Returns:
            èŒä½ä¿¡æ¯å­—å…¸æˆ–None
        """
        try:
            # ç­‰å¾…job cardå®¹å™¨åŠ è½½
            await page.wait_for_selector('div.DkhPwc', timeout=5000)
            
            # æå–jobId (ä»URLä¸­æå–è¿ç»­5ä½ä»¥ä¸Šæ•°å­—)
            current_url = page.url
            job_id_match = re.search(r'\d{5,}', current_url)
            job_id = job_id_match.group(0) if job_id_match else None
            
            # æå–æ ‡é¢˜
            title_element = await page.query_selector('h2.p1N2lc')
            title = await title_element.inner_text() if title_element else ""
            
            # æå–åœ°ç‚¹
            location_elements = await page.query_selector_all('.r0wTof')
            locations = []
            for loc_elem in location_elements:
                loc_text = await loc_elem.inner_text()
                locations.append(loc_text.strip())
            location = '; '.join(locations)
            
            # æå–èŒä½çº§åˆ«
            level_element = await page.query_selector('.wVSTAb')
            level = await level_element.inner_text() if level_element else ""
            
            # æå–èŒä½æè¿°
            job_description = await self._extract_job_description(page)
            
            # æ„å»ºèŒä½æ•°æ®
            job_data = {
                'jobId': job_id,
                'title': title.strip(),
                'company': 'Google',
                'location': location,
                'level': level.strip(),
                'jobDescription': job_description,
                'url': current_url,
                'scraped_date': datetime.now().isoformat(),
                'source': 'google_career',
                'status': 'active'
            }
            
            return job_data
            
        except Exception as e:
            self.logger.error(f'âŒ æå–èŒä½æ•°æ®æ—¶å‡ºé”™: {e}')
            return None
    
    async def _extract_job_description(self, page: Page) -> str:
        """
        æå–èŒä½æè¿°
        
        Args:
            page: Playwrighté¡µé¢å¯¹è±¡
            
        Returns:
            æ ¼å¼åŒ–çš„èŒä½æè¿°
        """
        try:
            description_parts = []
            
            # æå–Minimum qualifications
            min_qual = await self._extract_section_content(page, 'Minimum qualifications')
            if min_qual:
                description_parts.extend(['Minimum qualifications:', min_qual, ''])
            
            # æå–Preferred qualifications
            pref_qual = await self._extract_section_content(page, 'Preferred qualifications')
            if pref_qual:
                description_parts.extend(['Preferred qualifications:', pref_qual, ''])
            
            # æå–About the job
            about_job = await self._extract_paragraph_content(page, 'About the job')
            if about_job:
                description_parts.extend(['About the job:', about_job, ''])
            
            # æå–Responsibilities
            responsibilities = await self._extract_section_content(page, 'Responsibilities')
            if responsibilities:
                description_parts.extend(['Responsibilities:', responsibilities])
            
            return '\n'.join(description_parts)
            
        except Exception as e:
            self.logger.error(f'âŒ æå–èŒä½æè¿°æ—¶å‡ºé”™: {e}')
            return ""
    
    async def _extract_section_content(self, page: Page, heading_text: str) -> str:
        """
        æå–ulåˆ—è¡¨å†…å®¹
        
        Args:
            page: Playwrighté¡µé¢å¯¹è±¡
            heading_text: ç« èŠ‚æ ‡é¢˜æ–‡æœ¬
            
        Returns:
            ç« èŠ‚å†…å®¹
        """
        try:
            # æŸ¥æ‰¾åŒ…å«æŒ‡å®šæ–‡æœ¬çš„h3æ ‡ç­¾
            h3_elements = await page.query_selector_all('h3')
            target_h3 = None
            
            for h3 in h3_elements:
                h3_text = await h3.inner_text()
                if heading_text.lower() in h3_text.lower():
                    target_h3 = h3
                    break
            
            if not target_h3:
                return ""
            
            # è·å–ä¸‹ä¸€ä¸ªå…„å¼Ÿå…ƒç´ 
            next_element = await page.evaluate(
                '(h3) => h3.nextElementSibling',
                target_h3
            )
            
            if next_element:
                tag_name = await page.evaluate('(el) => el.tagName', next_element)
                if tag_name == 'UL':
                    # æå–ulä¸­çš„æ‰€æœ‰liå†…å®¹
                    li_elements = await page.query_selector_all('li')
                    # è¿‡æ»¤å‡ºå±äºå½“å‰ulçš„liå…ƒç´ 
                    li_texts = []
                    ul_lis = await page.evaluate(
                        '(ul) => Array.from(ul.querySelectorAll("li")).map(li => li.textContent.trim())',
                        next_element
                    )
                    return '\n'.join(ul_lis)
            
            return ""
            
        except Exception as e:
            self.logger.error(f'âŒ æå–sectionå†…å®¹æ—¶å‡ºé”™: {e}')
            return ""
    
    async def _extract_paragraph_content(self, page: Page, heading_text: str) -> str:
        """
        æå–æ®µè½å†…å®¹
        
        Args:
            page: Playwrighté¡µé¢å¯¹è±¡
            heading_text: ç« èŠ‚æ ‡é¢˜æ–‡æœ¬
            
        Returns:
            æ®µè½å†…å®¹
        """
        try:
            # æŸ¥æ‰¾åŒ…å«æŒ‡å®šæ–‡æœ¬çš„h3æ ‡ç­¾
            h3_elements = await page.query_selector_all('h3')
            target_h3 = None
            
            for h3 in h3_elements:
                h3_text = await h3.inner_text()
                if heading_text.lower() in h3_text.lower():
                    target_h3 = h3
                    break
            
            if not target_h3:
                return ""
            
            # è·å–åç»­çš„æ®µè½å†…å®¹
            paragraphs = await page.evaluate('''
                (h3) => {
                    const paragraphs = [];
                    let current = h3.nextElementSibling;
                    while (current && current.tagName !== 'H3') {
                        if (current.tagName === 'P') {
                            paragraphs.push(current.textContent.trim());
                        }
                        current = current.nextElementSibling;
                    }
                    return paragraphs;
                }
            ''', target_h3)
            
            return '\n\n'.join(paragraphs)
            
        except Exception as e:
            self.logger.error(f'âŒ æå–paragraphå†…å®¹æ—¶å‡ºé”™: {e}')
            return ""
    
    def _log_progress(self, current: int, total: int) -> None:
        """
        è®°å½•è¿›åº¦
        
        Args:
            current: å½“å‰è¿›åº¦
            total: æ€»æ•°
        """
        percentage = (current / total) * 100
        self.logger.info(f'ğŸ“ˆ è¿›åº¦: {current}/{total} ({percentage:.1f}%)')
    
    async def _retry_operation(self, operation, max_retries: int = 3):
        """
        é‡è¯•æœºåˆ¶
        
        Args:
            operation: è¦æ‰§è¡Œçš„æ“ä½œ
            max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
        """
        for attempt in range(max_retries):
            try:
                return await operation()
            except Exception as e:
                if attempt == max_retries - 1:
                    raise e
                self.logger.warning(f'âš ï¸ æ“ä½œå¤±è´¥ï¼Œé‡è¯•ä¸­... ({attempt + 1}/{max_retries})')
                await asyncio.sleep(1)
    
    async def save_scraped_data(self, filename: str = "google_jobs.json") -> None:
        """
        ä¿å­˜çˆ¬å–çš„æ•°æ®
        
        Args:
            filename: ä¿å­˜çš„æ–‡ä»¶å
        """
        try:
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(self.scraped_jobs, f, ensure_ascii=False, indent=2)
            self.logger.info(f'âœ… æ•°æ®å·²ä¿å­˜åˆ° {filename}')
            self.logger.info(f'ğŸ“Š å…±çˆ¬å– {len(self.scraped_jobs)} ä¸ªèŒä½')
        except Exception as e:
            self.logger.error(f'âŒ ä¿å­˜æ•°æ®æ—¶å‡ºé”™: {e}')


# æµ‹è¯•å’Œè¿è¡Œå‡½æ•°
async def main():
    """æµ‹è¯•å‡½æ•°"""
    try:
        # æ¨¡æ‹Ÿé…ç½®
        website_config = {
            "website_info": {
                "base_url": "https://www.google.com/about/careers/"
            },
            "scraping_config": {
                "max_pages": 5
            },
            "playwright_options": {
                "wait_for_selector": "li.zE6MFb",
                "wait_timeout": 10000
            }
        }
        
        global_config = {
            "global_settings": {
                "headless": False
            },
            "playwright_global": {
                "timeout": 30000,
                "user_agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
            }
        }
        
        # åˆ›å»ºæŠ“å–å™¨å®ä¾‹
        scraper = GoogleCareerScraper(website_config, global_config)
        
        # æ‰§è¡ŒæŠ“å–
        jobs = await scraper.scrape_jobs()
        
        # ä¿å­˜æ•°æ®
        await scraper.save_scraped_data()
        
        print(f"æŠ“å–å®Œæˆï¼å…±è·å¾— {len(jobs)} ä¸ªèŒä½")
        
    except Exception as e:
        print(f"æ‰§è¡Œè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")


if __name__ == "__main__":
    asyncio.run(main())